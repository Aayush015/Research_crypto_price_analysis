{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aayush015/Research_crypto_price_analysis/blob/main/Crypto_Price_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVqxcoGfflj0"
      },
      "source": [
        "# Description of the Project\n",
        "\n",
        "This project aims to get crypto currency price predictions for the year 2024 based on news media sentiments, and reddit posts sentiments from the year 2017 - 2023. The idea is that we will collect hourly 10 popular reddit posts from 2017 - 2023, and collect hourly 10 popular news for four cryptos: bitcon, etherum, shiba-inu, and dogecoin. We will use this data with the data we collected for hourly crypto price data, and train a machine learning model to learn patterns. We will use this model to forecast the data for 2024, and test it against actual crypto fluctuations for 2024."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pDefyTbz_Tt"
      },
      "source": [
        "# Data Collection\n",
        "First, we will collect crypto price data, reddit data, and traditional news data each from their own APIs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rEQ3BgvIzvN6",
        "outputId": "40e475e0-807c-49e8-b7a3-13c69c551de4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching hourly data for bitcoin...\n",
            "Data saved to bitcoin_hourly_2021-10-12_to_2023-12-19.csv\n",
            "Fetching hourly data for ethereum...\n",
            "Data saved to ethereum_hourly_2021-10-12_to_2023-12-19.csv\n",
            "Fetching hourly data for dogecoin...\n",
            "Data saved to dogecoin_hourly_2021-10-12_to_2023-12-19.csv\n",
            "Fetching hourly data for shiba-inu...\n",
            "Data saved to shiba-inu_hourly_2021-10-12_to_2023-12-19.csv\n",
            "Fetching hourly data for uniswap...\n",
            "Data saved to uniswap_hourly_2021-10-12_to_2023-12-19.csv\n",
            "Fetching hourly data for aave...\n",
            "Data saved to aave_hourly_2021-10-12_to_2023-12-19.csv\n",
            "Fetching hourly data for compound...\n",
            "Data saved to compound_hourly_2021-10-12_to_2023-12-19.csv\n",
            "Fetching hourly data for flow...\n",
            "Data saved to flow_hourly_2021-10-12_to_2023-12-19.csv\n",
            "Fetching hourly data for decentraland...\n",
            "Data saved to decentraland_hourly_2021-10-12_to_2023-12-19.csv\n",
            "Fetching hourly data for the-sandbox...\n",
            "Data saved to the-sandbox_hourly_2021-10-12_to_2023-12-19.csv\n",
            "Hourly data collection completed.\n"
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python\n",
        "import requests\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "import time\n",
        "\n",
        "def fetch_crypto_data_hourly(crypto_id, start_date, end_date):\n",
        "    \"\"\"\n",
        "    Fetch hourly historical price data for a cryptocurrency from CoinCap API.\n",
        "\n",
        "    :param crypto_id: Cryptocurrency ID (e.g., 'bitcoin', 'ethereum').\n",
        "    :param start_date: Start date (datetime object).\n",
        "    :param end_date: End date (datetime object).\n",
        "    :return: List of historical hourly data for the cryptocurrency.\n",
        "    \"\"\"\n",
        "    url = f\"https://api.coincap.io/v2/assets/{crypto_id}/history\"\n",
        "    all_data = []\n",
        "\n",
        "    # Fetch data in intervals to avoid API limitations\n",
        "    while start_date < end_date:\n",
        "        interval_end = min(start_date + timedelta(days=30), end_date)  # Fetch 1 month at a time\n",
        "        params = {\n",
        "            \"interval\": \"h1\",  # Hourly data\n",
        "            \"start\": int(start_date.timestamp() * 1000),  # Start timestamp in ms\n",
        "            \"end\": int(interval_end.timestamp() * 1000),  # End timestamp in ms\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            response = requests.get(url, params=params)\n",
        "            response.raise_for_status()\n",
        "            data = response.json().get(\"data\", [])\n",
        "            # Add the data to the collection\n",
        "            all_data.extend(data)\n",
        "        except requests.exceptions.RequestException as ex:\n",
        "            print(f\"Error fetching data for {crypto_id}: {ex}\")\n",
        "            break\n",
        "\n",
        "        start_date = interval_end + timedelta(hours=1)  # Move to the next interval\n",
        "        time.sleep(1)  # Sleep to respect API rate limits\n",
        "\n",
        "    return all_data\n",
        "\n",
        "def save_to_csv(data, filename):\n",
        "    \"\"\"\n",
        "    Save data to a CSV file.\n",
        "\n",
        "    :param data: List of historical price data.\n",
        "    :param filename: Output CSV filename.\n",
        "    \"\"\"\n",
        "    if not data:\n",
        "        print(\"No data to save.\")\n",
        "        return\n",
        "\n",
        "    # Convert the data to a pandas DataFrame\n",
        "    df = pd.DataFrame(data)\n",
        "    # Ensure timestamps are in human-readable format\n",
        "    df[\"time\"] = pd.to_datetime(df[\"time\"], unit=\"ms\")\n",
        "    # Rename columns for clarity\n",
        "    df.rename(columns={\"priceUsd\": \"price_usd\"}, inplace=True)\n",
        "    df.to_csv(filename, index=False)\n",
        "    print(f\"Data saved to {filename}\")\n",
        "\n",
        "def run():\n",
        "    \"\"\"\n",
        "    Main function to fetch and save hourly historical data for specified cryptocurrencies.\n",
        "    \"\"\"\n",
        "    # Cryptocurrencies of interest\n",
        "    cryptos = {\n",
        "        \"bitcoin\": \"bitcoin\",\n",
        "        \"ethereum\": \"ethereum\",\n",
        "        \"dogecoin\": \"dogecoin\",\n",
        "        \"shiba-inu\": \"shiba-inu\",\n",
        "        \"uniswap\": \"uniswap\",\n",
        "        \"aave\": \"aave\",\n",
        "        \"compound\": \"compound\",\n",
        "        \"flow\": \"flow\",\n",
        "        \"decentraland\": \"decentraland\",\n",
        "        \"the-sandbox\": \"the-sandbox\",\n",
        "    }\n",
        "\n",
        "    # Start and end dates\n",
        "    start_date = datetime(2021, 10, 12)  # Start date (YYYY, MM, DD)\n",
        "    end_date = datetime(2023, 12, 19)    # End date (YYYY, MM, DD)\n",
        "\n",
        "    for crypto_name, crypto_id in cryptos.items():\n",
        "        print(f\"Fetching hourly data for {crypto_name}...\")\n",
        "        data = fetch_crypto_data_hourly(crypto_id, start_date, end_date)\n",
        "\n",
        "        # Add a column identifying the cryptocurrency\n",
        "        for entry in data:\n",
        "            entry[\"crypto\"] = crypto_name\n",
        "\n",
        "        # Save data to a CSV file\n",
        "        filename = f\"{crypto_name}_hourly_2021-10-12_to_2023-12-19.csv\"\n",
        "        save_to_csv(data, filename)\n",
        "\n",
        "    print(\"Hourly data collection completed.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Merge Altcoins into one, Nfts into one, and Defi into one."
      ],
      "metadata": {
        "id": "JJD88Fuxbn0l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "# Define category mappings\n",
        "altcoin_coins = [\"shiba-inu\", \"dogecoin\"]\n",
        "nft_coins = [\"flow\", \"decentraland\", \"the-sandbox\"]\n",
        "defi_coins = [\"uniswap\", \"aave\", \"compound\"]\n",
        "standalone_coins = [\"bitcoin\", \"ethereum\"]\n",
        "\n",
        "# Function to load and process price data for each category\n",
        "def load_price_data(coin_list, category_label):\n",
        "    combined_df = pd.DataFrame()\n",
        "    for coin in coin_list:\n",
        "        file_path = f\"{coin}_hourly_2021-10-12_to_2023-12-19.csv\"\n",
        "        df = pd.read_csv(file_path)\n",
        "        df[\"time\"] = pd.to_datetime(df[\"time\"])\n",
        "        df[\"category\"] = category_label\n",
        "        df[\"coin\"] = coin\n",
        "        combined_df = pd.concat([combined_df, df], ignore_index=True)\n",
        "    return combined_df\n",
        "\n",
        "# Process each category\n",
        "altcoin_data = load_price_data(altcoin_coins, \"altcoin\")\n",
        "nft_data = load_price_data(nft_coins, \"nft\")\n",
        "defi_data = load_price_data(defi_coins, \"defi\")\n",
        "bitcoin_data = load_price_data([\"bitcoin\"], \"bitcoin\")\n",
        "ethereum_data = load_price_data([\"ethereum\"], \"ethereum\")\n",
        "\n",
        "# Combine all categories into a single dataset\n",
        "all_price_data = pd.concat([altcoin_data, nft_data, defi_data, bitcoin_data, ethereum_data], ignore_index=True)\n",
        "\n",
        "# Save combined price data to a file\n",
        "all_price_data.to_csv(\"combined_price_data.csv\", index=False)\n",
        "print(\"Combined price data saved to 'combined_price_data.csv'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U3ffiu1Ebmzp",
        "outputId": "4eb3e094-46bf-4bc4-a207-90cf7d70045e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined price data saved to 'combined_price_data.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Nr1FHy-jGsk"
      },
      "source": [
        "## Merge News and Price data\n",
        "\n",
        "* Preprocess news data (sentiment components get extracted into a separated columns using eval()).\n",
        "* Round news data to its nearest hour."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import ast  # For safely evaluating the sentiment dictionary string\n",
        "\n",
        "# Load the datasets\n",
        "news_data = pd.read_csv(\"cryptonews.csv\")\n",
        "price_data = pd.read_csv(\"combined_price_data.csv\")\n",
        "\n",
        "# Ensure proper datetime format\n",
        "news_data['date'] = pd.to_datetime(news_data['date'], format='mixed', errors='coerce')  # Handle mixed formats\n",
        "price_data['time'] = pd.to_datetime(price_data['time'])\n",
        "\n",
        "# Round news timestamps to the nearest hour\n",
        "news_data['rounded_time'] = news_data['date'].dt.round('h')\n",
        "\n",
        "# Parse the sentiment dictionary into separate columns\n",
        "def parse_sentiment(sentiment_str):\n",
        "    try:\n",
        "        sentiment_dict = ast.literal_eval(sentiment_str)  # Safely evaluate the string\n",
        "        return pd.Series({\n",
        "            \"sentiment_class\": sentiment_dict.get(\"class\"),\n",
        "            \"sentiment_polarity\": sentiment_dict.get(\"polarity\"),\n",
        "            \"sentiment_subjectivity\": sentiment_dict.get(\"subjectivity\")\n",
        "        })\n",
        "    except:\n",
        "        return pd.Series({\n",
        "            \"sentiment_class\": None,\n",
        "            \"sentiment_polarity\": None,\n",
        "            \"sentiment_subjectivity\": None\n",
        "        })\n",
        "\n",
        "# Apply parsing function\n",
        "sentiment_data = news_data['sentiment'].apply(parse_sentiment)\n",
        "news_data = pd.concat([news_data, sentiment_data], axis=1)\n",
        "\n",
        "# Merge the news data with price data\n",
        "merged_data = pd.merge(price_data, news_data, left_on='time', right_on='rounded_time', how='left')\n",
        "\n",
        "# Save the merged dataset\n",
        "merged_data.to_csv(\"merged_price_news_data.csv\", index=False)\n",
        "print(\"Merged data saved to 'merged_price_news_data.csv'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "stZfoC3vcuJg",
        "outputId": "155d548a-f4f0-49cb-efa4-8fe6d59352c4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Merged data saved to 'merged_price_news_data.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Handle Missing Values"
      ],
      "metadata": {
        "id": "uyQ9rBmzgpJg"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "35vmO7EDgxlL"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPcVCJv63z/xhIU2KWoGKsG",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}