{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPosN5fLmHvE9f3i8ecAVHv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Aayush015/Research_crypto_price_analysis/blob/main/Crypto_Price_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Collection\n",
        "First, we will collect crypto price data, reddit data, and traditional news data each from their own APIs."
      ],
      "metadata": {
        "id": "-pDefyTbz_Tt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rEQ3BgvIzvN6",
        "outputId": "cd9b148b-3277-4c2b-88e3-29b3ba88c6a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching hourly data for bitcoin...\n",
            "Data saved to bitcoin_hourly_2017_to_2023.csv\n",
            "Fetching hourly data for ethereum...\n",
            "Data saved to ethereum_hourly_2017_to_2023.csv\n",
            "Fetching hourly data for dogecoin...\n",
            "Data saved to dogecoin_hourly_2017_to_2023.csv\n",
            "Fetching hourly data for shiba-inu...\n",
            "Data saved to shiba-inu_hourly_2017_to_2023.csv\n",
            "Hourly data collection completed.\n"
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python\n",
        "import requests\n",
        "import pandas as pd\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "def fetch_crypto_data_hourly(crypto_id, start_date, end_date):\n",
        "    \"\"\"\n",
        "    Fetch hourly historical price data for a cryptocurrency from CoinCap API.\n",
        "\n",
        "    :param crypto_id: Cryptocurrency ID (e.g., 'bitcoin', 'ethereum').\n",
        "    :param start_date: Start date (datetime object).\n",
        "    :param end_date: End date (datetime object).\n",
        "    :return: List of historical hourly data for the cryptocurrency.\n",
        "    \"\"\"\n",
        "    url = f\"https://api.coincap.io/v2/assets/{crypto_id}/history\"\n",
        "    all_data = []\n",
        "\n",
        "    # Fetch data in intervals to avoid API limitations\n",
        "    while start_date < end_date:\n",
        "        interval_end = min(start_date + timedelta(days=30), end_date)  # Fetch 1 month at a time\n",
        "        params = {\n",
        "            \"interval\": \"h1\",  # Hourly data\n",
        "            \"start\": int(start_date.timestamp() * 1000),  # Start timestamp in ms\n",
        "            \"end\": int(interval_end.timestamp() * 1000),  # End timestamp in ms\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            response = requests.get(url, params=params)\n",
        "            response.raise_for_status()\n",
        "            data = response.json().get(\"data\", [])\n",
        "            # Add the data to the collection\n",
        "            all_data.extend(data)\n",
        "        except requests.exceptions.RequestException as ex:\n",
        "            print(f\"Error fetching data for {crypto_id}: {ex}\")\n",
        "            break\n",
        "\n",
        "        start_date = interval_end + timedelta(hours=1)  # Move to the next interval\n",
        "\n",
        "    return all_data\n",
        "\n",
        "def save_to_csv(data, filename):\n",
        "    \"\"\"\n",
        "    Save data to a CSV file.\n",
        "\n",
        "    :param data: List of historical price data.\n",
        "    :param filename: Output CSV filename.\n",
        "    \"\"\"\n",
        "    if not data:\n",
        "        print(\"No data to save.\")\n",
        "        return\n",
        "\n",
        "    # Convert the data to a pandas DataFrame\n",
        "    df = pd.DataFrame(data)\n",
        "    # Ensure timestamps are in human-readable format\n",
        "    df[\"time\"] = pd.to_datetime(df[\"time\"], unit=\"ms\")\n",
        "    # Rename columns for clarity\n",
        "    df.rename(columns={\"priceUsd\": \"price_usd\"}, inplace=True)\n",
        "    df.to_csv(filename, index=False)\n",
        "    print(f\"Data saved to {filename}\")\n",
        "\n",
        "def run():\n",
        "    \"\"\"\n",
        "    Main function to fetch and save hourly historical data for specified cryptocurrencies.\n",
        "    \"\"\"\n",
        "    cryptos = [\"bitcoin\", \"ethereum\", \"dogecoin\", \"shiba-inu\"]  # Cryptocurrencies of interest\n",
        "    start_date = datetime(2017, 1, 1)  # Start of historical data\n",
        "    end_date = datetime(2023, 12, 31)  # End of historical data\n",
        "\n",
        "    for crypto in cryptos:\n",
        "        print(f\"Fetching hourly data for {crypto}...\")\n",
        "        data = fetch_crypto_data_hourly(crypto, start_date, end_date)\n",
        "\n",
        "        # Add a column identifying the cryptocurrency\n",
        "        for entry in data:\n",
        "            entry[\"crypto\"] = crypto\n",
        "\n",
        "        # Save data to a CSV file\n",
        "        filename = f\"{crypto}_hourly_2017_to_2023.csv\"\n",
        "        save_to_csv(data, filename)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run()\n",
        "    print(\"Hourly data collection completed.\")"
      ]
    }
  ]
}